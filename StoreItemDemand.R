####################################################################################################
####################################################################################################
# Store Item Demand (Kaggle)                                             ###########################
# Ryan Wolff                                                             ###########################
# 13 November 2023                                                       ###########################
# Data Location and Description:                                         ###########################
# https://www.kaggle.com/competitions/demand-forecasting-kernels-only/overview #####################
####################################################################################################
####################################################################################################

########################################################################
########################################################################
#############################
# Start run in parallel
# cl <- makePSOCKcluster(3)
# registerDoParallel(cl)

# End run in parallel
# stopCluster(cl)
#############################
########################################################################
########################################################################

#################################################################
#################################################################
# EDA                                               #############
#################################################################
#################################################################

library(vroom)
library(tidyverse)
library(timetk)
library(patchwork)

train <- vroom("train.csv")
test <- vroom("test.csv")

# Time Series Plots for 4 different store-item combos
train[train$store == 1 & train$item == 1, ] %>%
  plot_time_series(date, sales, .interactive = FALSE)

train[train$store == 2 & train$item == 2, ] %>%
  plot_time_series(date, sales, .interactive = FALSE)

train[train$store == 3 & train$item == 3, ] %>%
  plot_time_series(date, sales, .interactive = FALSE)

train[train$store == 4 & train$item == 4, ] %>%
  plot_time_series(date, sales, .interactive = FALSE)

# ACF (autocorrelation function plots) for the same
# 4 different store-item combos
acf_1_1 <- train[train$store == 1 & train$item == 1, ] %>%
  pull(sales) %>%
  forecast::ggAcf(.) + ggtitle("ACF for Store 1 Item 1")

acf_2_2 <- train[train$store == 2 & train$item == 2, ] %>%
  pull(sales) %>%
  forecast::ggAcf(.) + ggtitle("ACF for Store 2 Item 2")

acf_3_3 <- train[train$store == 3 & train$item == 3, ] %>%
  pull(sales) %>%
  forecast::ggAcf(.) + ggtitle("ACF for Store 3 Item 3")

acf_4_4 <- train[train$store == 4 & train$item == 4, ] %>%
  pull(sales) %>%
  forecast::ggAcf(.) + ggtitle("ACF for Store 4 Item 4")

# Four-way plot of ACF plots to show differences
# in the autocorrelation structure for the different
# items and justify treating the problem as 200
# different time series problems (one for each
# store-item combo)
fourway_acf <- (acf_1_1 + acf_2_2) / (acf_3_3 + acf_4_4)
fourway_acf

###################################################################################
###################################################################################
# Feature Engineering and Random Forest for a Single Store-Item Combo #############
###################################################################################
###################################################################################

### Load Data and Packages ###

# Packages
library(vroom)
library(tidyverse)
library(timetk)
library(patchwork)
library(embed)
library(lubridate)
library(parsnip)
library(ranger)
library(workflows)
library(tidymodels)
library(poissonreg)
library(rpart)
library(stacks)
library(dbarts)
library(xgboost)


# Data
train <- vroom("train.csv")
test <- vroom("test.csv")

# Subset store-item combo w my favorite numbers
storeItem <- train %>%
  filter(store == 4, item == 17)



### Light EDA ###

# Time Series Plot
tsp <- storeItem %>%
  plot_time_series(date, sales, .interactive = FALSE)

# ACF (autocorrelation function plots)
acf <- storeItem %>%
  pull(sales) %>%
  forecast::ggAcf(.) + ggtitle("ACF for Store 4 Item 4")

# ACF-Year
acf_y <- storeItem %>%
  pull(sales) %>%
  forecast::ggAcf(., lag.max = 365) + ggtitle("ACF for Store 4 Item 4")

# ACF-All Time
acf_all <- storeItem %>%
  pull(sales) %>%
  forecast::ggAcf(., lag.max = 5*365) + ggtitle("ACF for Store 4 Item 4")

# Four-Way Plot
fourway <- (tsp + acf) / (acf_y + acf_all)
fourway

# EDA Findings:
# General increase in sales
# Heavy weekly autocorrelation (7 days)
# Biggest sales in the summer
# Smallest sales in the winter



### Recipe ###

# Create Recipe
rec <- recipe(sales ~ ., data = storeItem) %>%
  step_rm(c('store', 'item')) %>%
  step_date(date, features = c('week', 'month', 'quarter')) %>%
  step_mutate_at(all_integer_predictors(), fn = factor) %>%
  step_mutate(season = factor(case_when(
    between(month(date), 3, 5) ~ "Spring",
    between(month(date), 6, 8) ~ "Summer",
    between(month(date), 9, 11) ~ "Fall",
    TRUE ~ "Winter"
  ))) %>%
  step_mutate(cumulative_sales = cumsum(sales)) %>%
  step_dummy(all_nominal_predictors()) # Make nominal predictors into dummy variables

# Prep, Bake, and View Recipe
prepped <- prep(rec)
bake(prepped, storeItem)



### Model: Random Forest ###

# Model
rf_model <- rand_forest(mtry = tune(),
                              min_n = tune(),
                              trees = 500) %>% # Type of Model
  set_engine("ranger") %>% # What R function to use
  set_mode("regression")


# Workflow
rf_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model)

# Tuning grid
tuning_grid <- grid_regular(mtry(range = c(1, 7)), # Grid of values to tune over
                            min_n(),
                            levels = 5) # levels = L means L^2 total tuning possibilities

folds <- vfold_cv(storeItem, # Split data for CV
                  v = 5, # 5 folds
                  repeats = 1)

# Run CV
cv_results <- rf_workflow %>%
  tune_grid(resamples = folds,
            grid = tuning_grid,
            metrics = metric_set(smape))

# Find Best Tuning Parameters
best_tune <- cv_results %>%
  select_best("smape")
best_tune
# mtry = 7, min_n = 2

cv_results %>% collect_metrics() %>%
  filter(.metric == "smape")
# mean for best_tune is 18.6

#################################################################
#################################################################
# Exponential Smoothing for a Single Store-Item Combo ###########
#################################################################
#################################################################

# Load Libraries
library(vroom)
library(tidyverse)
library(modeltime)
library(timetk)
library(tidymodels)
library(patchwork)




# Get Data

# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")

# Subset two store-item combos w my favorite numbers
s4_i17 <- train %>%
  filter(store == 4, item == 17)

s6_i13 <- train %>%
  filter(store == 6, item == 13)




# Cross Validation

# CV for store 4 item 17
cv_split_4_17 <- time_series_split(s4_i17,
                                   assess = "3 months",
                                   cumulative = TRUE)
cv_preds_4_17 <- cv_split_4_17 %>%
  tk_time_series_cv_plan() %>% # put into data frame
  plot_time_series_cv_plan(date, sales, .interactive = FALSE)
cv_preds_4_17

# CV for store 6 item 13
cv_split_6_13 <- time_series_split(s6_i13,
                                   assess = "3 months",
                                   cumulative = TRUE)
cv_preds_6_13 <- cv_split_6_13 %>%
  tk_time_series_cv_plan() %>% # put into data frame
  plot_time_series_cv_plan(date, sales, .interactive = FALSE)
cv_preds_6_13



# Exponential smoothing

# ES for store 4 item 17
es_model_4_17 <- exp_smoothing() %>%
  set_engine('ets') %>%
  fit(sales~date, data = training(cv_split_4_17))

# Cross-validate to tune model
cv_results_4_17 <- modeltime_calibrate(es_model_4_17,
                                       new_data = testing(cv_split_4_17))

# Visualize CV results
cv_results_vis_4_17 <- cv_results_4_17 %>%
  modeltime_forecast(
    new_data = testing(cv_split_4_17),
    actual_data = s4_i17
  ) %>%
  plot_modeltime_forecast(.interactive = FALSE)
cv_results_vis_4_17

# ES for store 6 item 13
es_model_6_13 <- exp_smoothing() %>%
  set_engine('ets') %>%
  fit(sales~date, data = training(cv_split_6_13))

# Cross-validate to tune model
cv_results_6_13 <- modeltime_calibrate(es_model_6_13,
                                       new_data = testing(cv_split_6_13))

# Visualize CV results
cv_results_vis_6_13 <- cv_results_6_13 %>%
  modeltime_forecast(
    new_data = testing(cv_split_6_13),
    actual_data = s6_i13
  ) %>%
  plot_modeltime_forecast(.interactive = FALSE)
cv_results_vis_6_13




# Refit to all data then forecast for store 4 item 17

es_fullfit_4_17 <- cv_results_4_17 %>%
  modeltime_refit(data = s4_i17)

es_preds_4_17 <- es_fullfit_4_17 %>%
  modeltime_forecast(h = '3 months') %>%
  rename(date = .index, sales = .value)%>%
  select(date, sales) %>%
  full_join(., y = test, by = "date") %>%
  select(id, sales)
  
es_fullfit_plot_4_17 <- es_fullfit_4_17 %>%
  modeltime_forecast(h = '3 months', actual_data = s4_i17) %>%
  plot_modeltime_forecast(.interactive = FALSE)
es_fullfit_plot_4_17

# Refit to all data then forecast for store 6 item 13
es_fullfit_6_13 <- cv_results_6_13 %>%
  modeltime_refit(data = s6_i13)

es_preds_6_13 <- es_fullfit_6_13 %>%
  modeltime_forecast(h = '3 months') %>%
  rename(date = .index, sales = .value)%>%
  select(date, sales) %>%
  full_join(., y = test, by = "date") %>%
  select(id, sales)
  
es_fullfit_plot_6_13 <- es_fullfit_6_13 %>%
  modeltime_forecast(h = '3 months', actual_data = s6_i13) %>%
  plot_modeltime_forecast(.interactive = FALSE)
es_fullfit_plot_6_13




# Plots
plotly::subplot(cv_results_vis_4_17, cv_results_vis_6_13, es_fullfit_plot_4_17, es_fullfit_plot_6_13, nrows = 2)

# Four-Way Plot
fourway <- (cv_results_vis_4_17 + cv_results_vis_6_13) / (es_fullfit_plot_4_17 + es_fullfit_plot_6_13)
fourway

#################################################################
#################################################################
# SARIMA for a Couple Store-Item Combos               ###########
#################################################################
#################################################################

# Load Libraries
library(vroom)
library(tidyverse)
library(modeltime)
library(timetk)
library(tidymodels)
library(patchwork)
library(forecast)
library(embed)
library(lubridate)
library(parsnip)
library(workflows)
library(ggplot2)




# Get Data

# Load Data
train <- vroom("train.csv")
test <- vroom("test.csv")

# Subset two store-item combos w my favorite numbers
s4_i17 <- train %>%
  filter(store == 4, item == 17)

s6_i13 <- train %>%
  filter(store == 6, item == 13)

test_s4_i17 <- test %>%
  filter(store == 4, item == 17)

test_s6_i13 <- test %>%
  filter(store == 6, item == 13)




# Recipe for Linear Model Part
# Create Recipe
rec <- recipe(sales ~ ., data = s4_i17) %>%
  step_rm(c('store', 'item')) %>%
  step_mutate_at(all_integer_predictors(), fn = factor) %>%
  step_mutate(cumulative_sales = cumsum(sales)) %>%
  step_dummy(all_nominal_predictors()) # Make nominal predictors into dummy variables

# Prep, Bake, and View Recipe
prepped <- prep(rec)
bake(prepped, s4_i17)




# Cross Validation Splits

# CV for store 4 item 17
cv_split_4_17 <- time_series_split(s4_i17,
                                   assess = "3 months",
                                   cumulative = TRUE)
cv_preds_4_17 <- cv_split_4_17 %>%
  tk_time_series_cv_plan() %>% # put into data frame
  plot_time_series_cv_plan(date, sales, .interactive = FALSE)
cv_preds_4_17

# CV for store 6 item 13
cv_split_6_13 <- time_series_split(s6_i13,
                                   assess = "3 months",
                                   cumulative = TRUE)
cv_preds_6_13 <- cv_split_6_13 %>%
  tk_time_series_cv_plan() %>% # put into data frame
  plot_time_series_cv_plan(date, sales, .interactive = FALSE)
cv_preds_6_13




# ARIMA

# ARIMA Model
arima_model <- arima_reg(seasonal_period = 365,
                         non_seasonal_ar = 5, # default max p to tune
                         non_seasonal_ma = 5, # default max q to tune
                         seasonal_ar = 2, # default max P to tune
                         seasonal_ma = 2, # default max Q to tune
                         non_seasonal_differences = 2, # default max d to tune
                         seasonal_differences = 2) %>% # default max D to tune
  set_engine('auto_arima')

# ARIMA Workflows
arima_wf_4_17 <- workflow() %>%
  add_recipe(rec) %>%
  add_model(arima_model) %>%
  fit(data = training(cv_split_4_17))

arima_wf_6_13 <- workflow() %>%
  add_recipe(rec) %>%
  add_model(arima_model) %>%
  fit(data = training(cv_split_6_13))





# Calibrate/Tune Workflows
cv_results_4_17 <- modeltime_calibrate(arima_wf_4_17,
                                       new_data = testing(cv_split_4_17))

cv_results_6_13 <- modeltime_calibrate(arima_wf_6_13,
                                       new_data = testing(cv_split_6_13))

# Visualize and Evaluate CV Accuracies
cv_results_vis_4_17 <- cv_results_4_17 %>%
  modeltime_forecast(
    new_data = testing(cv_split_4_17),
    actual_data = s4_i17
  ) %>%
  plot_modeltime_forecast(.interactive = FALSE) +
  labs(title = 'CV Predictions and True Obs, Store 4 Item 17')
cv_results_vis_4_17

cv_results_vis_6_13 <- cv_results_6_13 %>%
  modeltime_forecast(
    new_data = testing(cv_split_6_13),
    actual_data = s6_i13
  ) %>%
  plot_modeltime_forecast(.interactive = FALSE) +
  labs(title = 'CV Predictions and True Obs, Store 6 Item 13')
cv_results_vis_6_13




# Refit Best Model to Entire Data and Predict
arima_fullfit_4_17 <- cv_results_4_17 %>%
  modeltime_refit(data = s4_i17)

arima_forecast_plot_4_17 <- arima_fullfit_4_17 %>%
  modeltime_forecast(
    new_data = test_s4_i17,
    actual_data = s4_i17
  ) %>%
  plot_modeltime_forecast(.interactive = FALSE) +
  labs(title = '3-Month Forecast, Store 4 Item 17')
arima_forecast_plot_4_17

arima_fullfit_6_13 <- cv_results_6_13 %>%
  modeltime_refit(data = s6_i13)

arima_forecast_plot_6_13 <- arima_fullfit_6_13 %>%
  modeltime_forecast(
    new_data = test_s6_i13,
    actual_data = s6_i13
  ) %>%
  plot_modeltime_forecast(.interactive = FALSE) +
  labs(title = '3-Month Forecast, Store 6 Item 13')
arima_forecast_plot_6_13




# Plots
plotly::subplot(cv_results_vis_4_17, cv_results_vis_6_13, arima_forecast_plot_4_17, arima_forecast_plot_6_13, nrows = 2)

# Four-Way Plot
fourway <- (cv_results_vis_4_17 + cv_results_vis_6_13) / (arima_forecast_plot_4_17 + arima_forecast_plot_6_13)
fourway






























# For later use (making predictions)
####################################################

# Treat the problem as if we have 200
# time series (one for each store-item combo)
nStores <- max(train$store)
nItems <- max(train$item)
for(s in 11:nStores) {
  for(i in 1:nItems) {
    storeItemtrain <- train %>%
      filter(store == s, item == i)
    storeItemTest <- test %>%
      filter(store == s, item == i)
  }
  
  ## Fit storeItem models here
  
  ## Predict storeItem sales
  
  ## Save storeItem predictions
  
  if(s == 1 & i == 1) {
    all_preds <- preds
  } else {
    all_preds <- bind_rows(all_preds, preds)
  }
  
}
